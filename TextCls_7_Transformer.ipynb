{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TextCls_7_Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaoziove/TextClassification/blob/master/TextCls_7_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wd2YUWs1i239",
        "outputId": "3c54bed5-a3eb-4810-dd55-8d72f76f6db3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "!git clone https://github.com/yaoziove/TextClassification.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TextClassification'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 24 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IRYNabVp3wQK",
        "outputId": "a8e3ea28-eda9-4e45-fa42-2658a23a1411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/  \u001b[01;34mTextClassification\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xP01zY0730X9",
        "outputId": "c999b9df-29fa-481a-d7c4-9b565296d845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "cd TextClassification/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/TextClassification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1EDxl-T033gs",
        "outputId": "fb2ecd37-b1bb-44b2-b9d2-752ef3af5a57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mChinese-Text-Classification-Pytorch\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nGCaShnyG1YV",
        "outputId": "2ac73d6f-44e9-476c-c04c-08cca2c5dda3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "cd Chinese-Text-Classification-Pytorch/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/TextClassification/Chinese-Text-Classification-Pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i1_QfNX5G1es",
        "outputId": "a0fd93f2-8c72-4b42-d079-8b7ba04d4147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LICENSE  links.txt  \u001b[0m\u001b[01;34mmodels\u001b[0m/  README.md  run.py  \u001b[01;34mTHUCNews\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XYI3w2bO3_1X"
      },
      "source": [
        "#### 1.Utils\n",
        "data preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h-Lxyq_b34ZW",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "from tqdm import tqdm\n",
        "import time \n",
        "from datetime import timedelta\n",
        "\n",
        "MAX_VOCAB_SIZE = 10000   #词表长度限制\n",
        "UNK,PAD = '<UNK>','<PAD>' #未知字，padding字符\n",
        "\n",
        "def build_vocab(file_path,tokenizer,max_size,min_freq):\n",
        "  vocab_dic = {}\n",
        "  with open(file_path,\"r\",encoding=\"utf-8\") as f:\n",
        "    for line in tqdm(f):\n",
        "      line  = line.strip()\n",
        "      if not line:\n",
        "        continue\n",
        "      content = line.split('\\t')[0]\n",
        "      for word in tokenizer(content):\n",
        "        vocab_dic[word] = vocab_dic.get(word,0)+1\n",
        "    vocab_list = sorted([_ for _ in vocab_dic.items() if _[1]>=min_freq],key=lambda x:x[1],reverse=True)[:max_size]\n",
        "    vocab_dic = {word_count[0]:idx for idx,word_count in enumerate(vocab_list)}\n",
        "    vocab_dic.undate({UNK:len(vocab_dic),PAD:len(vocab_dic)+1})\n",
        "  return vocab_dic\n",
        "\n",
        "def build_dataset(config,ues_word):  #ues word 翻译为“词\"\n",
        "  if ues_word:\n",
        "    tokenizer = lambda x:x.split(' ')\n",
        "  else:\n",
        "    tokenizer = lambda x:[y for y in x] #char-leve\n",
        "\n",
        "  if os.path.exists(config.vocab_path):\n",
        "    vocab = pkl.load(open(config.vocab_path,'rb'))\n",
        "  else:\n",
        "    vocab = build_vocab(config.train_path,tokenizer=tokenizer,max_size=MAX_VOCAB_SIZE,min_freq=1)\n",
        "    pkl.dump(vocab,open(config.vocab_path,'wb'))\n",
        "  print(f\"Vocab size: {len(vocab)}\")\n",
        "\n",
        "  def load_dataset(path, pad_size=32):\n",
        "    contents = []\n",
        "    with open(path, 'r', encoding='UTF-8') as f:\n",
        "      for line in tqdm(f):\n",
        "        lin = line.strip()\n",
        "        if not lin:\n",
        "          continue\n",
        "        content, label = lin.split('\\t')\n",
        "        words_line = []\n",
        "        token = tokenizer(content)\n",
        "        seq_len = len(token)\n",
        "        if pad_size:\n",
        "          if len(token) < pad_size:\n",
        "            token.extend([PAD]*(pad_size-len(token)))\n",
        "          else:\n",
        "            token = token[:pad_size]\n",
        "            seq_len = pad_size\n",
        "        # word to id\n",
        "        for word in token:\n",
        "          words_line.append(vocab.get(word, vocab.get(UNK)))\n",
        "        contents.append((words_line, int(label), seq_len))\n",
        "    return contents\n",
        "  train = load_dataset(config.train_path, config.pad_size)\n",
        "  dev = load_dataset(config.dev_path, config.pad_size)\n",
        "  test = load_dataset(config.test_path, config.pad_size)\n",
        "  return vocab, train, dev, test\n",
        "\n",
        "class DatasetIterater(object):\n",
        "  def __init__(self, batches, batch_size, device):\n",
        "    self.batch_size = batch_size\n",
        "    self.batches = batches\n",
        "    self.n_batches = len(batches) // batch_size\n",
        "    self.residue = False  # 记录batch数量是否为整数\n",
        "    if len(batches) % self.n_batches != 0:\n",
        "        self.residue = True\n",
        "    self.index = 0\n",
        "    self.device = device\n",
        "\n",
        "  def _to_tensor(self, datas):\n",
        "    x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n",
        "    y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n",
        "\n",
        "    # pad前的长度(超过pad_size的设为pad_size)\n",
        "    seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n",
        "    return (x,seq_len),y\n",
        "\n",
        "  def __next__(self):\n",
        "    if self.residue and self.index == self.n_batches:\n",
        "      batches = self.batches[self.index*self.batch_size:len(self.batches)]\n",
        "      self.index += 1\n",
        "      batches = self._to_tensor(batches)\n",
        "      return batches\n",
        "\n",
        "    elif self.index >= self.n_batches:\n",
        "      self.index = 0\n",
        "      raise StopIteration\n",
        "    else:\n",
        "      batches = self.batches[self.index*self.batch_size:(self.index+1)*self.batch_size]\n",
        "      self.index += 1\n",
        "      batches = self._to_tensor(batches)\n",
        "      return batches\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self\n",
        "\n",
        "  def __len__(self):\n",
        "    if self.residue:\n",
        "      return self.n_batches + 1\n",
        "    else:\n",
        "      return self.n_batches\n",
        "\n",
        "def build_iterator(dataset, config):\n",
        "  iter_ = DatasetIterater(dataset, config.batch_size, config.device)\n",
        "  return iter_\n",
        "\n",
        "def get_time_dif(start_time):\n",
        "  \"\"\"获取已使用时间\"\"\"\n",
        "  end_time = time.time()\n",
        "  time_dif = end_time - start_time\n",
        "  return timedelta(seconds=int(round(time_dif)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i2EOPaF5DX8w"
      },
      "source": [
        "#### 2.Model 相关"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oorG02kjvxXW",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "class Config(object):\n",
        "  \"\"\"配置参数\"\"\"\n",
        "  def __init__(self,dataset,embedding):\n",
        "    self.model_name = 'Transformer'\n",
        "    self.train_path = dataset + '/data/train.txt'    #训练街\n",
        "    self.dev_path = dataset + '/data/dev.txt'      #验证集\n",
        "    self.test_path = dataset + '/data/test.txt'        #测试集\n",
        "    self.class_list = [x.strip() for x in open(dataset+'/data/class.txt',encoding='utf-8').readlines()]  #类别列表\n",
        "    self.vocab_path = dataset + '/data/vocab.pkl'    #词表\n",
        "    self.save_path = dataset + '/' + self.model_name + '.ckpt'  #模型训练结果\n",
        "    self.log_path = dataset + '/log/' + self.model_name\n",
        "    self.embedding_pretrained = torch.tensor(np.load(dataset + '/data/'+ \n",
        "                    embedding)[\"embeddings\"].astype('float32')) if embedding != 'random' else None\n",
        "    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    self.dropout = 0.5                    #随机失活\n",
        "    self.require_improvement = 1000             #若超过1000batch效果没提升，则提前结束\n",
        "    self.num_classes = len(self.class_list)         #类别数\n",
        "    self.n_vocab = 0                     #词表大小\n",
        "    self.num_epochs = 20                   #epoch数\n",
        "    self.batch_size = 128\n",
        "    self.pad_size = 32                    #每句话处理成的长度（短填长切）\n",
        "    self.learning_rate = 5e-4              \n",
        "    #字向量维度\n",
        "    self.embed = self.embedding_pretrained.size(1) if self.embedding_pretrained is not None else 300 \n",
        "    self.dim_model = 300\n",
        "    self.hidden = 1024\n",
        "    self.last_hidden = 512\n",
        "    self.num_head = 5\n",
        "    self.num_encoder = 2\n",
        "\n",
        "\"\"\"\n",
        "    Attetion is all you need:\n",
        "\"\"\"\n",
        "class Model(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super(Model,self).__init__()\n",
        "    if config.embedding_pretrained is not None:\n",
        "        self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained,freeze=False)\n",
        "    else:\n",
        "        self.embedding = nn.Embedding(config.n_vocab,config.embed,padding_idx=config.n_vocab - 1)\n",
        "\n",
        "    self.position_embedding = Positional_Encoding(config.embed, config.pad_size,\n",
        "                                                config.dropout, config.device)\n",
        "    self.encoder = Encoder(config.dim_model, config.num_head, config.hidden, config.dropout)\n",
        "    self.encoders = nn.ModuleList([\n",
        "        copy.deepcopy(self.encoder) for _ in range(config.num_encoder)\n",
        "    ])\n",
        "    self.fc1 = nn.Linear(config.pad_size*config.dim_model,config.num_classes)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    out = self.embedding(x[0])\n",
        "    out = self.position_embedding(out)\n",
        "    for encoder in self.encoders:\n",
        "        out = encoder(out)\n",
        "    out = out.view(out.size(0),-1)\n",
        "    out = self.fc1(out)\n",
        "    return out\n",
        "\n",
        "#一个完整的multi_head_attention+ Add_Norm + Feedforward+ Add_norm\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,dim_model,num_head,hidden,dropout):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.attention = Multi_Head_Attention(dim_model,num_head,dropout)\n",
        "    self.feed_forward = Position_wise_Feed_Forward(dim_model,hidden,dropout)\n",
        "      \n",
        "  def forward(self,x):\n",
        "    out = self.attention(x)\n",
        "    out = self.feed_forward(out)\n",
        "    return out\n",
        "    \n",
        "class Positional_Encoding(nn.Module):\n",
        "  def __init__(self,embed,pad_size,dropout,device):\n",
        "    super(Positional_Encoding,self).__init__()\n",
        "    self.device = device\n",
        "    self.pe = torch.tensor([[pos/(10000.0**(i//2*2.0 / embed)) for i in \n",
        "                            range(embed)] for pos in range(pad_size)])\n",
        "    self.pe[:,0::2] = np.sin(self.pe[:,0::2])\n",
        "    self.pe[:,1::2] = np.cos(self.pe[:,1::2])\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self,x):\n",
        "    out = x + nn.Parameter(self.pe,requires_grad=False).to(self.device)\n",
        "    out = self.dropout(out)\n",
        "    return out\n",
        "    \n",
        "class Scaled_Dot_Product_Attention(nn.Module):\n",
        "  \"\"\"scaled dot-product attention\"\"\"\n",
        "  def __init__(self):\n",
        "    super(Scaled_Dot_Product_Attention,self).__init__()\n",
        "  def forward(self,Q,K,V,scale=None):\n",
        "    '''\n",
        "    Args:\n",
        "        Q: [batch_size, len_Q, dim_Q]\n",
        "        K: [batch_size, len_K, dim_K]\n",
        "        V: [batch_size, len_V, dim_V]\n",
        "        scale: 缩放因子 论文为根号dim_K\n",
        "    Return:\n",
        "        self-attention后的张量，以及attention张量\n",
        "    '''\n",
        "    attention = torch.matmul(Q,K.permute(0,2,1))#[batch_size,len,len]\n",
        "    if scale:\n",
        "      attention = attention * scale\n",
        "#   if mask:\n",
        "#     attention = attention.masked_fill_(mask==0,-1e9)\n",
        "    attention = F.softmax(attention,dim=-1)\n",
        "    context = torch.matmul(attention,V)\n",
        "    return context\n",
        "    \n",
        "class Multi_Head_Attention(nn.Module):\n",
        "  def __init__(self,dim_model,num_head,dropout=0.0):\n",
        "    super(Multi_Head_Attention,self).__init__()\n",
        "    self.num_head = num_head\n",
        "    assert dim_model % num_head == 0\n",
        "    self.dim_head = dim_model // self.num_head\n",
        "    self.fc_Q = nn.Linear(dim_model, num_head * self.dim_head)\n",
        "    self.fc_K = nn.Linear(dim_model, num_head * self.dim_head)\n",
        "    self.fc_V = nn.Linear(dim_model, num_head * self.dim_head)\n",
        "    self.attention = Scaled_Dot_Product_Attention()\n",
        "    self.fc = nn.Linear(num_head*self.dim_head,dim_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.layer_norm = nn.LayerNorm(dim_model)\n",
        "      \n",
        "  def forward(self,x):\n",
        "    batch_size = x.size(0)\n",
        "    Q = self.fc_Q(x)\n",
        "    K = self.fc_K(x)\n",
        "    V = self.fc_V(x)\n",
        "    Q = Q.view(batch_size * self.num_head, -1, self.dim_head)\n",
        "    K = K.view(batch_size * self.num_head, -1, self.dim_head)\n",
        "    V = V.view(batch_size * self.num_head, -1, self.dim_head)\n",
        "    # if mask:  # TODO\n",
        "    #     mask = mask.repeat(self.num_head, 1, 1)  # TODO change this\n",
        "    scale = K.size(-1) ** -0.5  # 缩放因子\n",
        "    context = self.attention(Q, K, V, scale)\n",
        "    context = context.view(batch_size, -1, self.dim_head * self.num_head)\n",
        "    out = self.fc(context)\n",
        "    out = self.dropout(out)\n",
        "    out = out + x  # 残差连接\n",
        "    out = self.layer_norm(out)\n",
        "    return out\n",
        "\n",
        "class Position_wise_Feed_Forward(nn.Module):\n",
        "  def __init__(self, dim_model, hidden, dropout=0.0):\n",
        "    super(Position_wise_Feed_Forward, self).__init__()\n",
        "    self.fc1 = nn.Linear(dim_model, hidden)\n",
        "    self.fc2 = nn.Linear(hidden, dim_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.layer_norm = nn.LayerNorm(dim_model)\n",
        "      \n",
        "  def forward(self,x):\n",
        "    out = self.fc1(x)\n",
        "    out = F.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    out = self.dropout(out)\n",
        "    out = out + x  # 残差连接\n",
        "    out = self.layer_norm(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqmpCf4cXyXv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "3ea3de88-fa58-4268-868f-4e90ef4751ed"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "A = torch.rand(2,3,4)\n",
        "B = torch.rand(2,3,4)\n",
        "V = torch.rand(2,3,4)\n",
        "c = torch.matmul(A , B.permute(0,2,1))\n",
        "print(f'shape of c: {c.shape}')\n",
        "d = F.softmax(c,dim=1)\n",
        "print(f'shape of d: {d.shape}')\n",
        "e = torch.matmul(d,V)\n",
        "print(f'shape of e: {e.shape}')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of c: torch.Size([2, 3, 3])\n",
            "shape of d: torch.Size([2, 3, 3])\n",
            "shape of e: torch.Size([2, 3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9qSeGWT9Lbz_",
        "colab": {}
      },
      "source": [
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# a=torch.rand(2,3,4)\n",
        "# a=a.permute(0,2,1)\n",
        "# print(\"shape after permute:\",a.shape)\n",
        "# maxpool = nn.MaxPool1d(3)\n",
        "# a=maxpool(a)\n",
        "# print(\"shape after maxpool:\",a.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DytR2Ii1dlQa",
        "outputId": "9c2f6ce4-292e-44c0-f6dd-702207be6ec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (47.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PO8gSPzLDcea"
      },
      "source": [
        "#### 3.train and eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lGQMkeTK7pbk",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn import metrics\n",
        "import time\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "#权重初始化，默认Xavier\n",
        "def init_network(model,method='xavier',exclude='embedding',seed=123):\n",
        "  for name,w in model.named_parameters():\n",
        "    if exclude not in name:\n",
        "      if 'weight' in name:\n",
        "        if method == 'xavier':\n",
        "          nn.init.xavier_normal_(w)\n",
        "        elif method == 'kaiming':\n",
        "          nn.init.kaiming_normal_(w)\n",
        "        else:\n",
        "          nn.init.normal_(w)\n",
        "      elif 'bias' in name:\n",
        "        nn.init.constant_(w,0)\n",
        "      else:\n",
        "        pass\n",
        "\n",
        "def train(config,model,train_iter,dev_iter,test_iter):\n",
        "  start_time = time.time()\n",
        "  model.train()\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr=config.learning_rate)\n",
        "\n",
        "  #学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n",
        "  #scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "  total_batch = 0             #记录进行到多少batch \n",
        "  dev_best_loss = float('inf')      #验证集最好的loss、\n",
        "  last_improve = 0            #记录上次验证集loss下降的batch数\n",
        "  flag = False              #记录是否很久没有效果提升\n",
        "  writer = SummaryWriter(log_dir=config.log_path+'/'+time.strftime('%m-%d_%H.%M',time.localtime()))\n",
        "\n",
        "  for epoch in range(config.num_epochs):\n",
        "    print('Epoch [{}/{}]'.format(epoch+1,config.num_epochs))\n",
        "    #scheduler.step()           #学习率衰减\n",
        "\n",
        "    for i,(trains,labels) in enumerate(train_iter):\n",
        "      outputs = model(trains)\n",
        "      model.zero_grad()\n",
        "      loss = F.cross_entropy(outputs,labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if total_batch % 100 == 0:\n",
        "        #每多少轮输出在训练集和验证集上的效果\n",
        "        true = labels.data.cpu()\n",
        "        predic = torch.max(outputs.data,1)[1].cpu()\n",
        "\n",
        "        train_acc = metrics.accuracy_score(true,predic)\n",
        "        dev_acc,dev_loss = evaluate(config,model,dev_iter)\n",
        "        if dev_loss < dev_best_loss:\n",
        "          dev_best_loss = dev_loss\n",
        "          torch.save(model.state_dict(),config.save_path)\n",
        "          improve = '*'\n",
        "          last_improve = total_batch\n",
        "        else:\n",
        "          improve = ''\n",
        "        time_dif = get_time_dif(start_time)\n",
        "        msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n",
        "        print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n",
        "        writer.add_scalar(\"loss/train\", loss.item(), total_batch)\n",
        "        writer.add_scalar(\"loss/dev\", dev_loss, total_batch)\n",
        "        writer.add_scalar(\"acc/train\", train_acc, total_batch)\n",
        "        writer.add_scalar(\"acc/dev\", dev_acc, total_batch)\n",
        "        model.train()\n",
        "      total_batch += 1\n",
        "      if total_batch - last_improve > config.require_improvement:\n",
        "        # 验证集loss超过1000batch没下降，结束训练\n",
        "        print(\"No optimization for a long time, auto-stopping...\")\n",
        "        flag = True\n",
        "        break\n",
        "    if flag:\n",
        "      break\n",
        "  writer.close()\n",
        "  test(config,model,test_iter)\n",
        "\n",
        "def test(config,model,test_iter):\n",
        "  model.load_state_dict(torch.load(config.save_path))\n",
        "  model.eval()\n",
        "  start_tiem = time.time()\n",
        "  test_acc, test_loss, test_report, test_confusion = evaluate(config,model,test_iter,test=True)\n",
        "  msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n",
        "  print(msg.format(test_loss, test_acc))\n",
        "  print(\"Precision, Recall and F1-Score...\")\n",
        "  print(test_report)\n",
        "  print(\"Confusion Matrix...\")\n",
        "  print(test_confusion)\n",
        "  time_dif = get_time_dif(start_time)\n",
        "  print(\"Time usage:\", time_dif)\n",
        "\n",
        "def evaluate(config,model,data_iter,test=False):\n",
        "  model.eval()\n",
        "  loss_total=0\n",
        "  predict_all = np.array([],dtype=int)\n",
        "  labels_all = np.array([],dtype=int)\n",
        "  with torch.no_grad():\n",
        "    for texts,labels in data_iter:\n",
        "      outputs = model(texts)\n",
        "      loss = F.cross_entropy(outputs, labels)\n",
        "      loss_total += loss.item()\n",
        "      labels = labels.data.cpu().numpy()\n",
        "      predic = torch.max(outputs.data,1)[1].cpu()\n",
        "      labels_all = np.append(labels_all,labels)\n",
        "      predict_all = np.append(predict_all,predic)\n",
        "\n",
        "  acc = metrics.accuracy_score(labels_all,predict_all)\n",
        "  if test:\n",
        "    report = metrics.classification_report(labels_all,predict_all,target_names=config.class_list,digits=4)\n",
        "    confusion = metrics.confusion_matrix(labels_all,predict_all)\n",
        "    return acc,loss_total / len(data_iter),report,confusion\n",
        "\n",
        "  return acc,loss_total / len(data_iter)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DenvZrigODvR",
        "outputId": "b6e4ab03-72bb-4a1a-80de-88c4ead904f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "'''提取预训练词向量'''\n",
        "train_dir = \"./THUCNews/data/train.txt\"\n",
        "vocab_dir = \"./THUCNews/data/vocab.pkl\"\n",
        "pretrain_dir = \"./THUCNews/data/sgns.sogou.word\"\n",
        "emb_dim = 300\n",
        "filename_trimmed_dir = \"./THUCNews/data/embedding_SougouNews.npz\"\n",
        "\n",
        "if os.path.exists(vocab_dir):\n",
        "  word_to_id = pkl.load(open(vocab_dir, 'rb'))\n",
        "else:\n",
        "  tokenizer = lambda x:[y for y in x]  #以字为单位构建词表\n",
        "  word_to_id = build_vocab(train_dir,tokenizer=tokenizer,max_size=MAX_VOCAB_SIZE,min_freq=1)\n",
        "  pkl.dump(word_to_id,open(vocab_dir,'wb'))\n",
        "\n",
        "print(f'shape of word_to_is is {len(word_to_id)}')\n",
        "\n",
        "#通过预训练的词向量来表示词表数据\n",
        "# embeddings = np.random.rand(len(word_to_id),emb_dim)\n",
        "# f = open(pretrain_dir, \"r\", encoding='UTF-8')\n",
        "# for i, line in enumerate(f.readlines()):\n",
        "#   if i == 0:  # 若第一行是标题，则跳过\n",
        "#     continue\n",
        "#   lin = line.strip().split(\" \")\n",
        "#   if lin[0] in word_to_id:\n",
        "#     idx = word_to_id[lin[0]]\n",
        "#     emb = [float(x) for x in lin[1:301]]\n",
        "#     embeddings[idx] = np.asarray(emb, dtype='float32')\n",
        "# f.close()\n",
        "# np.savez_compressed(filename_trimmed_dir, embeddings=embeddings)\n",
        "\n",
        "#载入通过预训练的到的词向量表示\n",
        "embeddings = np.load(filename_trimmed_dir)\n",
        "embeddings = embeddings[\"embeddings\"]\n",
        "print(f'shape of embedding vocab is : {embeddings.shape}')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of word_to_is is 4762\n",
            "shape of embedding vocab is : (4762, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TuaJjoAsRNnU",
        "outputId": "427ee1e9-f1fe-4da5-a720-1e45854cb90a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "dataset = 'THUCNews'\n",
        "embedding = 'embedding_SougouNews.npz'\n",
        "config = Config(dataset,embedding)\n",
        "\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "torch.cuda.manual_seed_all(1)\n",
        "torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n",
        "\n",
        "start_time = time.time()\n",
        "print(\"Loading data...\")\n",
        "\n",
        "vocab,train_data,dev_data,test_data = build_dataset(config,False)\n",
        "train_iter = build_iterator(train_data, config)\n",
        "dev_iter = build_iterator(dev_data, config)\n",
        "test_iter = build_iterator(test_data, config)\n",
        "time_dif = get_time_dif(start_time)\n",
        "print(\"Time usage:\", time_dif)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6463it [00:00, 64625.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Vocab size: 4762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "180000it [00:03, 57652.77it/s]\n",
            "10000it [00:00, 62489.91it/s]\n",
            "10000it [00:00, 68876.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time usage: 0:00:03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FivUvcxHgDwB",
        "outputId": "8d0b7956-66f3-4efb-84aa-03dabd51bbda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#train\n",
        "config.n_vocab = len(vocab)\n",
        "\n",
        "model = Model(config).to(config.device)\n",
        "# init_network(model)\n",
        "print(model.parameters)\n",
        "train(config,model,train_iter,dev_iter,test_iter)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.parameters of Model(\n",
            "  (embedding): Embedding(4762, 300)\n",
            "  (position_embedding): Positional_Encoding(\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (encoder): Encoder(\n",
            "    (attention): Multi_Head_Attention(\n",
            "      (fc_Q): Linear(in_features=300, out_features=300, bias=True)\n",
            "      (fc_K): Linear(in_features=300, out_features=300, bias=True)\n",
            "      (fc_V): Linear(in_features=300, out_features=300, bias=True)\n",
            "      (attention): Scaled_Dot_Product_Attention()\n",
            "      (fc): Linear(in_features=300, out_features=300, bias=True)\n",
            "      (dropout): Dropout(p=0.5, inplace=False)\n",
            "      (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (feed_forward): Position_wise_Feed_Forward(\n",
            "      (fc1): Linear(in_features=300, out_features=1024, bias=True)\n",
            "      (fc2): Linear(in_features=1024, out_features=300, bias=True)\n",
            "      (dropout): Dropout(p=0.5, inplace=False)\n",
            "      (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (encoders): ModuleList(\n",
            "    (0): Encoder(\n",
            "      (attention): Multi_Head_Attention(\n",
            "        (fc_Q): Linear(in_features=300, out_features=300, bias=True)\n",
            "        (fc_K): Linear(in_features=300, out_features=300, bias=True)\n",
            "        (fc_V): Linear(in_features=300, out_features=300, bias=True)\n",
            "        (attention): Scaled_Dot_Product_Attention()\n",
            "        (fc): Linear(in_features=300, out_features=300, bias=True)\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (feed_forward): Position_wise_Feed_Forward(\n",
            "        (fc1): Linear(in_features=300, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=300, bias=True)\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Encoder(\n",
            "      (attention): Multi_Head_Attention(\n",
            "        (fc_Q): Linear(in_features=300, out_features=300, bias=True)\n",
            "        (fc_K): Linear(in_features=300, out_features=300, bias=True)\n",
            "        (fc_V): Linear(in_features=300, out_features=300, bias=True)\n",
            "        (attention): Scaled_Dot_Product_Attention()\n",
            "        (fc): Linear(in_features=300, out_features=300, bias=True)\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (feed_forward): Position_wise_Feed_Forward(\n",
            "        (fc1): Linear(in_features=300, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=300, bias=True)\n",
            "        (dropout): Dropout(p=0.5, inplace=False)\n",
            "        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc1): Linear(in_features=9600, out_features=10, bias=True)\n",
            ")>\n",
            "Epoch [1/20]\n",
            "Iter:      0,  Train Loss:   2.4,  Train Acc: 10.94%,  Val Loss:   4.5,  Val Acc: 10.00%,  Time: 0:00:02 *\n",
            "Iter:    100,  Train Loss:   1.5,  Train Acc: 48.44%,  Val Loss:   1.1,  Val Acc: 62.22%,  Time: 0:00:08 *\n",
            "Iter:    200,  Train Loss:   1.1,  Train Acc: 57.81%,  Val Loss:   1.1,  Val Acc: 68.64%,  Time: 0:00:14 *\n",
            "Iter:    300,  Train Loss:  0.85,  Train Acc: 71.09%,  Val Loss:  0.83,  Val Acc: 75.38%,  Time: 0:00:20 *\n",
            "Iter:    400,  Train Loss:  0.87,  Train Acc: 75.78%,  Val Loss:  0.82,  Val Acc: 76.95%,  Time: 0:00:26 *\n",
            "Iter:    500,  Train Loss:  0.75,  Train Acc: 78.12%,  Val Loss:  0.85,  Val Acc: 77.37%,  Time: 0:00:33 \n",
            "Iter:    600,  Train Loss:  0.73,  Train Acc: 75.00%,  Val Loss:  0.72,  Val Acc: 79.19%,  Time: 0:00:39 *\n",
            "Iter:    700,  Train Loss:  0.68,  Train Acc: 77.34%,  Val Loss:  0.69,  Val Acc: 80.60%,  Time: 0:00:45 *\n",
            "Iter:    800,  Train Loss:  0.55,  Train Acc: 80.47%,  Val Loss:  0.75,  Val Acc: 79.27%,  Time: 0:00:51 \n",
            "Iter:    900,  Train Loss:  0.61,  Train Acc: 81.25%,  Val Loss:  0.68,  Val Acc: 81.70%,  Time: 0:00:57 *\n",
            "Iter:   1000,  Train Loss:  0.44,  Train Acc: 86.72%,  Val Loss:  0.65,  Val Acc: 82.31%,  Time: 0:01:03 *\n",
            "Iter:   1100,  Train Loss:  0.52,  Train Acc: 78.91%,  Val Loss:  0.59,  Val Acc: 83.38%,  Time: 0:01:09 *\n",
            "Iter:   1200,  Train Loss:  0.76,  Train Acc: 78.12%,  Val Loss:  0.67,  Val Acc: 82.06%,  Time: 0:01:15 \n",
            "Iter:   1300,  Train Loss:  0.64,  Train Acc: 78.12%,  Val Loss:  0.58,  Val Acc: 84.02%,  Time: 0:01:22 *\n",
            "Iter:   1400,  Train Loss:  0.83,  Train Acc: 76.56%,  Val Loss:  0.59,  Val Acc: 83.81%,  Time: 0:01:28 \n",
            "Epoch [2/20]\n",
            "Iter:   1500,  Train Loss:  0.62,  Train Acc: 82.81%,  Val Loss:  0.64,  Val Acc: 83.01%,  Time: 0:01:34 \n",
            "Iter:   1600,  Train Loss:  0.48,  Train Acc: 85.94%,  Val Loss:  0.67,  Val Acc: 82.20%,  Time: 0:01:40 \n",
            "Iter:   1700,  Train Loss:  0.46,  Train Acc: 87.50%,  Val Loss:  0.57,  Val Acc: 85.01%,  Time: 0:01:46 *\n",
            "Iter:   1800,  Train Loss:  0.42,  Train Acc: 89.84%,  Val Loss:  0.54,  Val Acc: 84.91%,  Time: 0:01:52 *\n",
            "Iter:   1900,  Train Loss:  0.59,  Train Acc: 80.47%,  Val Loss:  0.53,  Val Acc: 85.13%,  Time: 0:01:58 *\n",
            "Iter:   2000,  Train Loss:  0.56,  Train Acc: 83.59%,  Val Loss:  0.55,  Val Acc: 85.09%,  Time: 0:02:04 \n",
            "Iter:   2100,  Train Loss:  0.58,  Train Acc: 85.16%,  Val Loss:  0.59,  Val Acc: 84.35%,  Time: 0:02:10 \n",
            "Iter:   2200,  Train Loss:  0.46,  Train Acc: 85.94%,  Val Loss:   0.6,  Val Acc: 84.55%,  Time: 0:02:17 \n",
            "Iter:   2300,  Train Loss:  0.41,  Train Acc: 85.94%,  Val Loss:  0.53,  Val Acc: 85.00%,  Time: 0:02:23 \n",
            "Iter:   2400,  Train Loss:  0.53,  Train Acc: 83.59%,  Val Loss:  0.59,  Val Acc: 84.81%,  Time: 0:02:29 \n",
            "Iter:   2500,  Train Loss:  0.42,  Train Acc: 84.38%,  Val Loss:  0.58,  Val Acc: 84.75%,  Time: 0:02:35 \n",
            "Iter:   2600,  Train Loss:  0.55,  Train Acc: 84.38%,  Val Loss:  0.55,  Val Acc: 85.20%,  Time: 0:02:41 \n",
            "Iter:   2700,  Train Loss:  0.51,  Train Acc: 83.59%,  Val Loss:  0.49,  Val Acc: 86.46%,  Time: 0:02:47 *\n",
            "Iter:   2800,  Train Loss:  0.58,  Train Acc: 78.91%,  Val Loss:   0.5,  Val Acc: 86.23%,  Time: 0:02:53 \n",
            "Epoch [3/20]\n",
            "Iter:   2900,  Train Loss:  0.49,  Train Acc: 85.16%,  Val Loss:  0.53,  Val Acc: 85.48%,  Time: 0:02:59 \n",
            "Iter:   3000,  Train Loss:  0.46,  Train Acc: 81.25%,  Val Loss:  0.52,  Val Acc: 86.22%,  Time: 0:03:05 \n",
            "Iter:   3100,  Train Loss:  0.46,  Train Acc: 88.28%,  Val Loss:   0.5,  Val Acc: 86.41%,  Time: 0:03:11 \n",
            "Iter:   3200,  Train Loss:  0.77,  Train Acc: 82.03%,  Val Loss:   0.5,  Val Acc: 86.98%,  Time: 0:03:17 \n",
            "Iter:   3300,  Train Loss:  0.47,  Train Acc: 85.16%,  Val Loss:  0.48,  Val Acc: 86.76%,  Time: 0:03:24 *\n",
            "Iter:   3400,  Train Loss:  0.54,  Train Acc: 84.38%,  Val Loss:   0.5,  Val Acc: 86.84%,  Time: 0:03:30 \n",
            "Iter:   3500,  Train Loss:  0.46,  Train Acc: 82.03%,  Val Loss:  0.52,  Val Acc: 86.55%,  Time: 0:03:36 \n",
            "Iter:   3600,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:   0.5,  Val Acc: 87.16%,  Time: 0:03:42 \n",
            "Iter:   3700,  Train Loss:  0.61,  Train Acc: 77.34%,  Val Loss:  0.47,  Val Acc: 86.63%,  Time: 0:03:48 *\n",
            "Iter:   3800,  Train Loss:  0.49,  Train Acc: 82.03%,  Val Loss:  0.55,  Val Acc: 85.58%,  Time: 0:03:54 \n",
            "Iter:   3900,  Train Loss:  0.46,  Train Acc: 84.38%,  Val Loss:  0.48,  Val Acc: 86.59%,  Time: 0:04:00 \n",
            "Iter:   4000,  Train Loss:  0.39,  Train Acc: 89.06%,  Val Loss:   0.5,  Val Acc: 86.75%,  Time: 0:04:06 \n",
            "Iter:   4100,  Train Loss:  0.53,  Train Acc: 79.69%,  Val Loss:  0.47,  Val Acc: 86.85%,  Time: 0:04:12 *\n",
            "Iter:   4200,  Train Loss:  0.49,  Train Acc: 85.16%,  Val Loss:  0.49,  Val Acc: 86.90%,  Time: 0:04:18 \n",
            "Epoch [4/20]\n",
            "Iter:   4300,  Train Loss:  0.44,  Train Acc: 85.94%,  Val Loss:  0.44,  Val Acc: 87.82%,  Time: 0:04:24 *\n",
            "Iter:   4400,  Train Loss:  0.32,  Train Acc: 88.28%,  Val Loss:  0.48,  Val Acc: 87.39%,  Time: 0:04:30 \n",
            "Iter:   4500,  Train Loss:  0.48,  Train Acc: 87.50%,  Val Loss:  0.44,  Val Acc: 88.04%,  Time: 0:04:37 *\n",
            "Iter:   4600,  Train Loss:  0.42,  Train Acc: 83.59%,  Val Loss:  0.46,  Val Acc: 87.72%,  Time: 0:04:43 \n",
            "Iter:   4700,  Train Loss:  0.46,  Train Acc: 85.94%,  Val Loss:  0.45,  Val Acc: 87.86%,  Time: 0:04:49 \n",
            "Iter:   4800,  Train Loss:  0.32,  Train Acc: 90.62%,  Val Loss:  0.44,  Val Acc: 88.08%,  Time: 0:04:55 \n",
            "Iter:   4900,  Train Loss:  0.42,  Train Acc: 85.94%,  Val Loss:  0.47,  Val Acc: 87.15%,  Time: 0:05:01 \n",
            "Iter:   5000,  Train Loss:  0.42,  Train Acc: 88.28%,  Val Loss:  0.44,  Val Acc: 87.76%,  Time: 0:05:07 \n",
            "Iter:   5100,  Train Loss:  0.56,  Train Acc: 82.81%,  Val Loss:  0.44,  Val Acc: 87.86%,  Time: 0:05:13 \n",
            "Iter:   5200,  Train Loss:  0.46,  Train Acc: 84.38%,  Val Loss:  0.47,  Val Acc: 87.25%,  Time: 0:05:19 \n",
            "Iter:   5300,  Train Loss:  0.43,  Train Acc: 83.59%,  Val Loss:  0.47,  Val Acc: 87.23%,  Time: 0:05:25 \n",
            "Iter:   5400,  Train Loss:  0.72,  Train Acc: 80.47%,  Val Loss:  0.45,  Val Acc: 87.81%,  Time: 0:05:31 \n",
            "Iter:   5500,  Train Loss:  0.33,  Train Acc: 89.06%,  Val Loss:  0.42,  Val Acc: 88.31%,  Time: 0:05:37 *\n",
            "Iter:   5600,  Train Loss:  0.38,  Train Acc: 89.06%,  Val Loss:  0.45,  Val Acc: 87.45%,  Time: 0:05:43 \n",
            "Epoch [5/20]\n",
            "Iter:   5700,  Train Loss:  0.44,  Train Acc: 86.72%,  Val Loss:  0.43,  Val Acc: 88.28%,  Time: 0:05:50 \n",
            "Iter:   5800,  Train Loss:  0.23,  Train Acc: 92.97%,  Val Loss:  0.49,  Val Acc: 87.19%,  Time: 0:05:56 \n",
            "Iter:   5900,  Train Loss:  0.37,  Train Acc: 86.72%,  Val Loss:  0.43,  Val Acc: 87.81%,  Time: 0:06:02 \n",
            "Iter:   6000,  Train Loss:  0.39,  Train Acc: 89.84%,  Val Loss:  0.45,  Val Acc: 87.99%,  Time: 0:06:08 \n",
            "Iter:   6100,  Train Loss:  0.46,  Train Acc: 86.72%,  Val Loss:  0.44,  Val Acc: 87.91%,  Time: 0:06:14 \n",
            "Iter:   6200,  Train Loss:  0.38,  Train Acc: 89.84%,  Val Loss:  0.44,  Val Acc: 88.30%,  Time: 0:06:20 \n",
            "Iter:   6300,  Train Loss:  0.35,  Train Acc: 89.84%,  Val Loss:  0.44,  Val Acc: 88.34%,  Time: 0:06:26 \n",
            "Iter:   6400,  Train Loss:  0.23,  Train Acc: 92.97%,  Val Loss:  0.41,  Val Acc: 88.72%,  Time: 0:06:32 *\n",
            "Iter:   6500,  Train Loss:  0.35,  Train Acc: 87.50%,  Val Loss:  0.45,  Val Acc: 87.99%,  Time: 0:06:38 \n",
            "Iter:   6600,  Train Loss:   0.4,  Train Acc: 89.06%,  Val Loss:  0.47,  Val Acc: 87.35%,  Time: 0:06:44 \n",
            "Iter:   6700,  Train Loss:  0.39,  Train Acc: 85.16%,  Val Loss:  0.41,  Val Acc: 88.25%,  Time: 0:06:50 \n",
            "Iter:   6800,  Train Loss:  0.36,  Train Acc: 88.28%,  Val Loss:  0.44,  Val Acc: 88.39%,  Time: 0:06:56 \n",
            "Iter:   6900,  Train Loss:  0.28,  Train Acc: 89.06%,  Val Loss:  0.42,  Val Acc: 88.11%,  Time: 0:07:02 \n",
            "Iter:   7000,  Train Loss:  0.37,  Train Acc: 84.38%,  Val Loss:  0.41,  Val Acc: 87.98%,  Time: 0:07:09 *\n",
            "Epoch [6/20]\n",
            "Iter:   7100,  Train Loss:  0.34,  Train Acc: 88.28%,  Val Loss:  0.42,  Val Acc: 88.69%,  Time: 0:07:15 \n",
            "Iter:   7200,  Train Loss:  0.51,  Train Acc: 83.59%,  Val Loss:  0.42,  Val Acc: 88.74%,  Time: 0:07:21 \n",
            "Iter:   7300,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.39,  Val Acc: 89.12%,  Time: 0:07:27 *\n",
            "Iter:   7400,  Train Loss:   0.5,  Train Acc: 82.81%,  Val Loss:  0.43,  Val Acc: 88.69%,  Time: 0:07:33 \n",
            "Iter:   7500,  Train Loss:  0.39,  Train Acc: 89.84%,  Val Loss:  0.41,  Val Acc: 88.46%,  Time: 0:07:39 \n",
            "Iter:   7600,  Train Loss:  0.28,  Train Acc: 90.62%,  Val Loss:  0.43,  Val Acc: 88.31%,  Time: 0:07:45 \n",
            "Iter:   7700,  Train Loss:  0.47,  Train Acc: 82.03%,  Val Loss:  0.44,  Val Acc: 87.93%,  Time: 0:07:51 \n",
            "Iter:   7800,  Train Loss:   0.5,  Train Acc: 82.03%,  Val Loss:  0.42,  Val Acc: 88.57%,  Time: 0:07:57 \n",
            "Iter:   7900,  Train Loss:  0.35,  Train Acc: 87.50%,  Val Loss:  0.41,  Val Acc: 88.61%,  Time: 0:08:03 \n",
            "Iter:   8000,  Train Loss:  0.33,  Train Acc: 88.28%,  Val Loss:   0.4,  Val Acc: 88.82%,  Time: 0:08:09 \n",
            "Iter:   8100,  Train Loss:  0.28,  Train Acc: 89.06%,  Val Loss:  0.41,  Val Acc: 88.40%,  Time: 0:08:15 \n",
            "Iter:   8200,  Train Loss:  0.42,  Train Acc: 88.28%,  Val Loss:  0.41,  Val Acc: 89.06%,  Time: 0:08:21 \n",
            "Iter:   8300,  Train Loss:  0.31,  Train Acc: 92.19%,  Val Loss:  0.41,  Val Acc: 88.33%,  Time: 0:08:28 \n",
            "No optimization for a long time, auto-stopping...\n",
            "Test Loss:  0.37,  Test Acc: 89.25%\n",
            "Precision, Recall and F1-Score...\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      finance     0.8974    0.8480    0.8720      1000\n",
            "       realty     0.8963    0.9330    0.9143      1000\n",
            "       stocks     0.8133    0.7930    0.8030      1000\n",
            "    education     0.9192    0.9550    0.9367      1000\n",
            "      science     0.7874    0.8370    0.8114      1000\n",
            "      society     0.9030    0.9030    0.9030      1000\n",
            "     politics     0.8878    0.8700    0.8788      1000\n",
            "       sports     0.9662    0.9730    0.9696      1000\n",
            "         game     0.9142    0.9060    0.9101      1000\n",
            "entertainment     0.9458    0.9070    0.9260      1000\n",
            "\n",
            "     accuracy                         0.8925     10000\n",
            "    macro avg     0.8930    0.8925    0.8925     10000\n",
            " weighted avg     0.8930    0.8925    0.8925     10000\n",
            "\n",
            "Confusion Matrix...\n",
            "[[848  18  82   6  26   8   7   1   2   2]\n",
            " [ 11 933  16   3  15  12   4   1   2   3]\n",
            " [ 58  37 793   2  65   3  26   2  12   2]\n",
            " [  0   4   5 955   5   8   7   1   5  10]\n",
            " [ 10   9  29  13 837  19  27   2  40  14]\n",
            " [  3  13   3  27  11 903  25   2   6   7]\n",
            " [ 10  12  26  14  22  32 870   4   4   6]\n",
            " [  0   2   2   3   5   4   6 973   0   5]\n",
            " [  3   5  12   4  58   3   4   2 906   3]\n",
            " [  2   8   7  12  19   8   4  19  14 907]]\n",
            "Time usage: 0:08:41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CxyhL5wb58NY",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}